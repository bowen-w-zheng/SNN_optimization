# SNOPS-JAX: Spiking Network Optimization using Population Statistics

A high-performance JAX/GPU implementation of **SNOPS** (Spiking Network Optimization using Population Statistics) from the paper:

> Wu, S., et al. (2024). "Automated customization of large-scale spiking network models to neuronal population activity." *Nature Computational Science*, 4, 690-705.

This Python implementation translates the original MATLAB code to JAX for GPU acceleration, modern ODE solvers, and scalable Bayesian optimization.

## ðŸš€ Features

- **GPU-accelerated simulation**: JAX-based spiking network simulator with multiple integrators (Euler, Heun, RK4, Diffrax)
- **Two network architectures**:
  - **CBN** (Classical Balanced Network): Random E/I connectivity
  - **SBN** (Spatial Balanced Network): Distance-dependent connectivity on 2D grid
- **Comprehensive statistics**: Firing rate, Fano factor, spike count correlations, Factor Analysis (shared variance, dimensionality, eigenspectrum)
- **Advanced Bayesian Optimization**:
  - ARD MatÃ©rn-5/2 Gaussian Processes
  - Constrained Expected Improvement with feasibility GP
  - Intensification with variance reduction
  - Multi-start L-BFGS-B acquisition optimization
- **Paper-faithful implementation**: Matches MATLAB baseline with <5% tolerance

## ðŸ“¦ Installation

### Requirements

- Python â‰¥3.9
- CUDA-compatible GPU (optional but recommended)

### Install from source

```bash
# Clone the repository
cd SNN_optimization

# Install in development mode
pip install -e .

# For GPU support (CUDA 12.x)
pip install -e ".[gpu]"

# For development tools
pip install -e ".[dev]"
```

### Dependencies

Core: `jax`, `jaxlib`, `numpy`, `scipy`, `pandas`, `h5py`, `pyyaml`
JAX tools: `equinox`, `optax`, `jaxopt`, `chex`, `diffrax`
GP/BO: `tinygp`, `scikit-learn`
Visualization: `matplotlib`, `seaborn`

## ðŸŽ¯ Quick Start

### Example 1: Simple Simulation

```python
import jax
from snops_jax.models.eif import EIFParams
from snops_jax.models.synapses import SynapticParams
from snops_jax.models.connectivity import build_cbn, NetworkParams
from snops_jax.simulate.run import run_simulation, SimulationConfig
from snops_jax.stats.single_pair import compute_statistics_summary

# Setup
n_e, n_i, n_ff = 800, 200, 200
eif_params = EIFParams()
syn_params = SynapticParams(tau_ed=5.0, tau_id=5.0)
network_params = NetworkParams(J_ee=20.0, J_ei=-40.0, J_ie=30.0, J_ii=-30.0)

# Build network
rng_key = jax.random.PRNGKey(42)
conn = build_cbn(n_e, n_i, n_ff, network_params, rng_key=rng_key)

# Simulate
sim_config = SimulationConfig(duration=10000.0, dt=0.05)
output = run_simulation(n_e, n_i, n_ff, conn, sim_config, eif_params, syn_params, rng_key)

# Compute statistics
stats = compute_statistics_summary(output.spike_counts_e)
print(f"Firing rate: {stats['fr']:.2f} sp/s, Fano factor: {stats['ff']:.2f}")
```

Run the example script:

```bash
python examples/simple_simulation.py
```

### Example 2: Compute Statistics from Data

```python
import jax.numpy as jnp
from snops_jax.stats.cost import compute_target_statistics, CostConfig

# Load spike count data (n_neurons, n_bins) from multiple sessions
spike_counts_sessions = [...]  # List of arrays

# Compute target statistics
config = CostConfig()
target = compute_target_statistics(spike_counts_sessions, config)

print(f"Target: fr={target.fr_mean:.2f}Â±{jnp.sqrt(target.fr_var):.2f} sp/s")
```

### Example 3: Run SNOPS Optimization (Coming Soon)

```python
from snops_jax.bo.driver import run_bo_loop, BOConfig
import yaml

# Load config
with open("snops_jax/config/default_cbn.yaml") as f:
    config_dict = yaml.safe_load(f)

# Define objective function
def objective(theta, rep):
    # theta: parameter array
    # rep: repetition index
    # Returns: (cost, stats, is_feasible)
    ...

# Run optimization
bounds = jnp.array([[1, 25], [1, 25], ...])  # Parameter bounds
bo_config = BOConfig(n_init=50, max_iterations=200)
state = run_bo_loop(objective, bounds, bo_config, rng_key)
```

## ðŸ“ Repository Structure

```
snops_jax/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ eif.py              # EIF neuron dynamics
â”‚   â”œâ”€â”€ synapses.py         # Synaptic currents
â”‚   â””â”€â”€ connectivity.py     # CBN/SBN connectivity
â”œâ”€â”€ simulate/
â”‚   â”œâ”€â”€ step.py             # Time-stepping kernel
â”‚   â””â”€â”€ run.py              # Main simulation loops (lax.scan)
â”œâ”€â”€ stats/
â”‚   â”œâ”€â”€ binning.py          # Spike binning & subsampling
â”‚   â”œâ”€â”€ single_pair.py      # fr, ff, rsc statistics
â”‚   â”œâ”€â”€ fa_jax.py           # Factor Analysis (EM)
â”‚   â””â”€â”€ cost.py             # Cost function (eq. 7)
â”œâ”€â”€ bo/
â”‚   â”œâ”€â”€ gp.py               # Gaussian Process surrogates (tinygp)
â”‚   â”œâ”€â”€ acquisition.py      # EI, constrained EI
â”‚   â”œâ”€â”€ suggest.py          # L-BFGS-B multi-start
â”‚   â”œâ”€â”€ intensify.py        # Intensification & feasibility
â”‚   â””â”€â”€ driver.py           # Main BO loop
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ default_cbn.yaml    # CBN configuration
â”‚   â””â”€â”€ default_sbn.yaml    # SBN configuration
â””â”€â”€ cli/                    # Command-line tools (TBD)
```

## ðŸ§ª Model Details

### EIF Neuron (Exponential Integrate-and-Fire)

```
C_m dV/dt = -g_L(V - E_L) + g_LÂ·Î”TÂ·exp((V - V_T)/Î”T) + I(t)
```

**Default parameters** (from paper):
- Ï„_m = 15 ms, E_L = -60 mV, V_T = -50 mV, V_th = -10 mV
- Î”T = 2 mV, V_re = -65 mV, Ï„_ref = 1.5 ms

### Synaptic Currents

First-order exponential decay with spike-triggered increments:
```
ds/dt = -s/Ï„ + Î£ JÂ·Î´(t - t_spike)
```

**Free parameters**: Ï„_ed, Ï„_id âˆˆ [1, 25] ms

### Connectivity

**CBN**: Random connections with fixed probabilities
**SBN**: Gaussian distance-dependent probability with periodic boundary

**Free parameters**:
- Synaptic strengths: J_ee, J_ei, J_ie, J_ii, J_eF, J_iF âˆˆ [-150, 150] mV
- Spatial widths (SBN): Ïƒ_e, Ïƒ_i, Ïƒ_F âˆˆ [0, 0.25] mm

### Statistics & Cost Function

**Single/pairwise**:
- `fr`: Mean firing rate (sp/s)
- `ff`: Fano factor (var/mean of spike counts)
- `rsc`: Spike count correlation (Fisher z-transformed)

**Population (via Factor Analysis)**:
- `%sh`: Percent shared variance
- `dsh`: Dimensionality (# eigenvalues for 95% variance)
- `es`: Eigenspectrum

**Cost** (paper eq. 7):
```
c_S(Î¸) = (1/Î£w_j) Â· Î£ w_j Â· [(s_j(Î¸) - s_j^true)Â² / v_j^true]
```

### Bayesian Optimization

1. **Initialization**: Latin Hypercube Sampling (50 points)
2. **Surrogate**: ARD MatÃ©rn-5/2 GP with MLE hyperparameters
3. **Acquisition**: Constrained EI (paper eq. 12)
   ```
   CEI(Î¸) = Î¦((Î¼_g - 0.5)/Ïƒ_g) Â· EI(Î¸)
   ```
4. **Feasibility**: Short-run checks (fr < 1, ff > 5, dsh < 1 â†’ infeasible)
5. **Intensification**: R=5 repetitions, early stop if SD < 0.15

## ðŸ”§ Configuration

Configurations are in `snops_jax/config/`. Key sections:

### Network
```yaml
network:
  type: "CBN"  # or "SBN"
  n_e: 4500
  n_i: 1125
  n_ff: 1000
```

### Simulation
```yaml
simulation:
  dt: 0.05                  # Euler timestep (ms)
  integrator: "euler"       # "euler", "heun", "rk4"
  full_duration: 140500.0   # 140.5 seconds
  burn_in: 500.0
  bin_size: 200.0
```

### BO
```yaml
bo:
  n_init: 50
  max_iterations: 200
  max_reps: 5              # Intensification
  sd_threshold: 0.15
  min_fr: 1.0              # Feasibility thresholds
  max_ff: 5.0
```

## ðŸ“Š Validation

The implementation is designed to match the MATLAB baseline within Â±5% tolerance:

| Statistic | Tolerance |
|-----------|-----------|
| fr        | Â±0.05 sp/s |
| ff        | Â±0.05 |
| rsc (Fisher z) | Â±0.02 |
| %sh       | Â±1.0 pp |
| dsh       | Â±1 |
| eigenspectrum | â‰¤5% L2 error |

**Testing** (upcoming):
```bash
pytest snops_jax/tests/
```

## ðŸš§ Roadmap

- [x] Core simulation engine (EIF, synapses, connectivity)
- [x] Statistics computation (fr, ff, rsc, FA)
- [x] Cost function (eq. 7)
- [x] Bayesian optimization (GP, constrained EI, intensification)
- [x] Configuration system
- [ ] Complete CLI tools
- [ ] Unit tests & MATLAB regression tests
- [ ] Example notebooks
- [ ] Multi-GPU support (pjit/pmap)
- [ ] Diffrax integration with spike-time interpolation

## ðŸ“š References

**Paper**:
- Wu, S., et al. (2024). *Nature Computational Science*, 4, 690-705.
  DOI: [10.1038/s43588-024-00688-3](https://doi.org/10.1038/s43588-024-00688-3)

**Original MATLAB code**:
- [github.com/ShenghaoWu/SpikingNetworkOptimization](https://github.com/ShenghaoWu/SpikingNetworkOptimization)

**Related work**:
- Huang et al. FI_SpatialNet MEX kernels

## ðŸ“ Citation

If you use SNOPS-JAX in your research, please cite:

```bibtex
@article{wu2024snops,
  title={Automated customization of large-scale spiking network models to neuronal population activity},
  author={Wu, Shenghao and Huang, Chengcheng and Snyder, Adam C and Smith, Matthew A and Doiron, Brent and Yu, Byron M},
  journal={Nature Computational Science},
  volume={4},
  pages={690--705},
  year={2024},
  publisher={Nature Publishing Group}
}
```

## ðŸ“„ License

MIT License (see LICENSE file)

## ðŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ðŸ’¬ Contact

For questions or issues:
- Open a GitHub issue
- Refer to the original paper for methodological details

---

**Status**: ðŸš§ Alpha release - core functionality complete, extensive testing in progress
